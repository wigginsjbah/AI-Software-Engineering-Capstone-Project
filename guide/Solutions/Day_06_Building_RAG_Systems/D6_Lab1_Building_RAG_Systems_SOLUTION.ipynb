{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems (Solution)\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete code and explanations for building the multi-agent RAG system. It demonstrates how to use LangGraph to create increasingly complex and capable agentic workflows.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 22:22:52,877 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "**Explanation:**\n",
    "This function gathers all our project documents, loads them, splits them into manageable chunks, and creates a FAISS vector store. The vector store converts the text chunks into numerical embeddings, which allows for efficient semantic search. The function returns a `retriever` object, which is the component our agents will use to query the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 19 document splits...\n"
     ]
    }
   ],
   "source": [
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Explanation:**\n",
    "This is the simplest form of a LangGraph system. \n",
    "1.  **`AgentState`**: We define the 'state' of our graph using a `TypedDict`. This is the shared memory that all nodes in the graph can read from and write to.\n",
    "2.  **Nodes**: Each node is a Python function that performs an action. The `retrieve` node calls our retriever, and the `generate` node calls the LLM.\n",
    "3.  **Graph Definition**: We instantiate `StateGraph` and add our nodes. The `set_entry_point` and `add_edge` methods define the directed flow of the graph. `compile()` creates the runnable graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking Simple RAG Graph ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GENERATE ANSWER---\n",
      "Final Answer: The purpose of the project, as outlined in the Problem section of the PRD, is to address the inefficiencies in new-hire onboarding processes, which are currently manual, inconsistent, and dispersed across multiple systems. This situation results in a confusing experience for new hires, imposes a significant administrative burden on HR, and causes a lack of visibility for hiring managers. The project aims to eliminate these issues to improve new-hire productivity, reduce the risk of compliance errors, and enhance employee retention and engagement from day one.\n"
     ]
    }
   ],
   "source": [
    "class SimpleAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---NODE: RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---NODE: GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "workflow_v1 = StateGraph(SimpleAgentState)\n",
    "workflow_v1.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v1.add_node(\"GENERATE\", generate)\n",
    "workflow_v1.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v1.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "workflow_v1.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v1 = workflow_v1.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Simple RAG Graph ---\")\n",
    "inputs = {\"question\": \"What is the purpose of this project according to the PRD?\"}\n",
    "result = app_v1.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Explanation:**\n",
    "Adding a Grader agent prevents the system from trying to answer a question with irrelevant information. This directly combats hallucination and makes the RAG system more trustworthy by allowing it to gracefully say, 'I don't know,' instead of making something up.\n",
    "\n",
    "1.  **`GraderAgent` Node:** We create a new node whose sole purpose is to act as a 'grader'. It calls the LLM with a very specific prompt, asking for a 'yes' or 'no' answer on whether the retrieved documents are relevant.\n",
    "2.  **Conditional Edge:** This is the key concept. `workflow.add_conditional_edges` tells the graph to execute a function (`decide_to_generate`) after the `GRADE` node. This function checks the output of the grader and returns the name of the *next* node to execute. This allows for dynamic routing and makes the agent much smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking Grader Graph with a relevant question ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are not relevant. End process.\n",
      "Final Answer: Could not answer question.\n",
      "\n",
      "--- Invoking Grader Graph with an irrelevant question ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are not relevant. End process.\n",
      "Final Answer: Could not answer question.\n"
     ]
    }
   ],
   "source": [
    "class GraderAgentState(SimpleAgentState):\n",
    "    grade: str\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. Grade 'yes' or 'no'.\\n\\nRetrieved Document: {documents}\\n\\nUser Question: {question}\"\"\"\n",
    "    grade = llm.invoke(prompt).content\n",
    "    return {\"grade\": grade}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"---NODE: CONDITIONAL EDGE---\")\n",
    "    if state[\"grade\"].lower() == \"yes\":\n",
    "        print(\"DECISION: Documents are relevant. Proceed to generation.\")\n",
    "        return \"GENERATE\"\n",
    "    else:\n",
    "        print(\"DECISION: Documents are not relevant. End process.\")\n",
    "        return END\n",
    "\n",
    "workflow_v2 = StateGraph(GraderAgentState)\n",
    "workflow_v2.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v2.add_node(\"GRADE\", grade_documents)\n",
    "workflow_v2.add_node(\"GENERATE\", generate)\n",
    "\n",
    "workflow_v2.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v2.add_edge(\"RETRIEVE\", \"GRADE\")\n",
    "workflow_v2.add_conditional_edges(\"GRADE\", decide_to_generate)\n",
    "workflow_v2.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v2 = workflow_v2.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with a relevant question ---\")\n",
    "inputs = {\"question\": \"What database schema will we use?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with an irrelevant question ---\")\n",
    "inputs = {\"question\": \"What is the weather in Paris?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Explanation:**\n",
    "This is a highly advanced workflow that mimics a real research team.\n",
    "1.  **Specialized Retrievers:** We create two separate vector stores and retrievers. This specialization allows us to direct queries to the most relevant knowledge source.\n",
    "2.  **Router/PM Agent:** The `ProjectManagerAgent` acts as a 'router.' This is a highly efficient pattern. Instead of one giant agent searching through all documents, the router first makes a quick, low-cost decision to delegate the task to a specialized agent with a smaller, more relevant knowledge base. This improves both speed and accuracy.\n",
    "3.  **Graph Construction:** We build the most complex graph yet. The entry point is the router. Based on its decision, the graph flows to one of the two specialist researchers. Both of their paths then converge on the `SYNTHESIZE` node, which creates the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 12 document splits...\n",
      "Creating vector store from 7 document splits...\n",
      "\n",
      "--- Invoking Research Team with a PRD question ---\n",
      "---NODE: PROJECT MANAGER (ROUTER)---\n",
      "PM Decision: Route to PRD_RESEARCHER\n",
      "---NODE: PRD RESEARCHER---\n",
      "---NODE: SYNTHESIZE ANSWER---\n",
      "Final Answer: The main user personas for this application are:\n",
      "\n",
      "1. **Eager Emma (The New Hire):** A new employee who feels a mix of excitement and anxiety before her start date. She struggles to find consolidated information about her team, company culture, and initial tasks, leading to uncertainty and frequent questions to her future manager and HR.\n",
      "\n",
      "2. **Guiding Greg (The Hiring Manager):** A hiring manager aiming to help new hires succeed but is overwhelmed by administrative tasks. He manually creates a 30-60-90 day plan, tracks down paperwork, and answers logistical questions, which detracts from focusing on strategic integration.\n",
      "\n",
      "3. **Systematic Sam (The HR Coordinator):** An HR coordinator involved in the onboarding process, dealing with the administrative burden of managing manual, distributed systems.\n",
      "\n",
      "--- Invoking Research Team with a technical question ---\n",
      "---NODE: PROJECT MANAGER (ROUTER)---\n",
      "PM Decision: Route to TECH_RESEARCHER\n",
      "---NODE: TECH RESEARCHER---\n",
      "---NODE: SYNTHESIZE ANSWER---\n",
      "Final Answer: The columns in the `users` table are `id`, `name`, `email`, and `role`.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create specialized retrievers\n",
    "prd_retriever = create_knowledge_base([\"artifacts/day1_prd.md\"])\n",
    "tech_retriever = create_knowledge_base([\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"])\n",
    "\n",
    "class ResearchTeamState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# 2. Define the agent nodes\n",
    "def prd_researcher(state):\n",
    "    print(\"---NODE: PRD RESEARCHER---\")\n",
    "    documents = prd_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def tech_researcher(state):\n",
    "    print(\"---NODE: TECH RESEARCHER---\")\n",
    "    documents = tech_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def synthesize_answer(state):\n",
    "    print(\"---NODE: SYNTHESIZE ANSWER---\")\n",
    "    prompt = f\"Based on the following documents, create a concise answer to the user's question.\\n\\nQuestion: {state['question']}\\n\\nDocuments: {state['documents']}\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def project_manager_router(state):\n",
    "    print(\"---NODE: PROJECT MANAGER (ROUTER)---\")\n",
    "    prompt = f\"You are a project manager. Based on the user's question, should you route this to the PRD expert or the Technical expert? Answer with 'PRD_RESEARCHER' or 'TECH_RESEARCHER'.\\n\\nQuestion: {state['question']}\"\n",
    "    decision = llm.invoke(prompt).content\n",
    "    print(f\"PM Decision: Route to {decision}\")\n",
    "    if 'PRD_RESEARCHER' in decision:\n",
    "        return \"PRD_RESEARCHER\"\n",
    "    else:\n",
    "        return \"TECH_RESEARCHER\"\n",
    "\n",
    "# 3. Build the graph\n",
    "workflow_v3 = StateGraph(ResearchTeamState)\n",
    "workflow_v3.add_node(\"PRD_RESEARCHER\", prd_researcher)\n",
    "workflow_v3.add_node(\"TECH_RESEARCHER\", tech_researcher)\n",
    "workflow_v3.add_node(\"SYNTHESIZE\", synthesize_answer)\n",
    "\n",
    "workflow_v3.add_conditional_edges(\"__start__\", project_manager_router)\n",
    "workflow_v3.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"SYNTHESIZE\", END)\n",
    "\n",
    "app_v3 = workflow_v3.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a PRD question ---\")\n",
    "inputs = {\"question\": \"What are the main user personas for this application?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a technical question ---\")\n",
    "inputs = {\"question\": \"What columns are in the users table?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
